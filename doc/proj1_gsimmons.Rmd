---
title: "R Notebook"
output:
  html_document: default
  html_notebook: default
  pdf_document: default
---

# Load necessary libraries
```{r, message=FALSE, warning=FALSE}
packages.used=c("rvest", "tibble", "qdap", 
                "sentimentr", "gplots", "dplyr",
                "tm", "syuzhet", "factoextra", 
                "beeswarm", "scales", "RColorBrewer",
                "RANN", "tm", "topicmodels","stringr","maps","countrycode","rworldmap")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

# load packages
library("rvest")
library("tibble")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")
library("tidytext")
library("ggplot2")
library("stringr")
library("maps")
library("countrycode")
library("rworldmap")

source("../lib/plotstacked.R")
source("../lib/speechFuncs.R")
```
# Get meta data from xlsx file
```{r}
library("xlsx")
library("rJava")
library("xlsxjars")

inaug =read.xlsx(file.path("..","data","InaugurationInfo.xlsx",fsep = .Platform$file.sep),1)
```
# get meta data needed for corpus (author, party affiliation, term)
```{r}
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
inaug_dates=f.speechlinks(main.page)
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
inaug_dates=f.speechlinks(main.page)
as.Date(inaug_dates[,1], format="%B %e, %Y")
inaug$Date=inaug_dates[-nrow(inaug_dates),]
inaug$Date=inaug$Date$links
inaug
```
# Let's add the full text to the inaug data frame
```{r, warning=FALSE}
ds = DirSource(directory = file.path("..","data","InauguralSpeeches",fsep = .Platform$file.sep),
               pattern = "*.txt",
               mode = "text")

for(i in seq(nrow(inaug))) {
  speech_text = paste(readLines(ds$filelist[i], n=-1, skipNul=TRUE, warn = FALSE), collapse = " ")
  inaug$text[i]=speech_text
  inaug$doc[i] = PlainTextDocument(speech_text,
                          heading = ds$filelist[i],
                          meta = list(author=c(inaug$President[i]),term=c(inaug$Term),date=c(inaug$Date),
                                      party=c(inaug$Party)),
                          id = c(i),
                          language = "en")
}
```
# Let's get our inaugural speeches into a corpus
```{r}
library(tm)
ds = DirSource(directory = file.path("..","data","InauguralSpeeches",fsep = .Platform$file.sep),
               pattern = "*.txt",
               mode = "text")
control_list <- list(removePunctuation = TRUE,
                     removeWords = stopwords("en"),
                     tolower = TRUE,
                     removeNumbers = TRUE,
                     stripWhitespace = TRUE,
                     PlainTextDocument = TRUE)
speech.corpus = VCorpus(ds, readerControl = control_list)
speech.tdm = TermDocumentMatrix(speech.corpus, control = control_list)
inspect(speech.tdm[5000:5020,1:2])

sentence.list=NULL
for(i in 1:nrow(inaug)){
  sentences=sent_detect(inaug$fulltext[i],
                        endmarks = c("?", ".", "!", "|",";"))
  if(length(sentences)>0){
    word.count=word_count(sentences)
    sentence.list=rbind(sentence.list, 
                        cbind(inaug[i,-ncol(inaug)],
                              sentences=as.character(sentences), 
                              word.count,
                              sent.id=1:length(sentences)
                              )
    )
  }
}
```
# Display a speech
```{r text_df, dependson="text"}
text = c(inaug$doc[[5]])
text_df <- data_frame(text)
head(text_df)
```
# Let's unnest tokens from speeches
```{r R.options = list(dplyr.print_max = 10)}
# create a tibble
tb = tibble(author=inaug$President,
            party=inaug$Party,
            text=inaug$text,
            date=inaug$Date,
            term=inaug$Term,
            file=inaug$File)

word_tb = unnest_tokens(tb,words,text,token="words")
word_tb
```
# Now let's remove stop words and clean up the tibble
```{r}
data(stop_words)
clean_word_tb <-anti_join(word_tb,stop_words,by=c("words"="word"))
clean_word_tb$words <-removePunctuation(clean_word_tb$words,preserve_intra_word_dashes = TRUE)
clean_word_tb
```
# Let's see which words appear the most (excluding stop words)
```{r}
clean_word_tb %>%
  count(words, sort=TRUE)
```
# Let's visualize that
```{r}
clean_word_tb %>%
  count(words, sort = TRUE) %>%
  filter(n > 150) %>%
  mutate(word = reorder(words, n)) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat = "identity") +
  xlab(NULL) +
  coord_flip()
```

# Now let's see how Obama and Trump compare
```{r}

tidy_comparison <- bind_rows(trump <- filter(clean_word_tb, author=="Donald J. Trump"),
                             clinton <- filter(clean_word_tb, author=="William J. Clinton"))

obama_percent <- clean_word_tb %>%
  mutate(word = str_extract(words, "[a-z']+")) %>%
  count(word) %>%
  transmute(word, obama = n / sum(n))

frequency <- tidy_comparison %>%
  mutate(word = str_extract(words, "[a-z']+")) %>%
  count(author, word) %>%
  mutate(other = n / sum(n)) %>%
  left_join(obama_percent) %>%
  ungroup()
```
# make the plot showing relative word frequencies between Trump and Obama and Nixon and Obama
```{r}
library(scales)

ggplot(frequency, aes(x = other, y = obama, color = abs(obama - other))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Barack Obama", x = NULL)
```

# Let's examine the correlation between Obama and Trump and Obama and Clinton
```{r}
cor.test(data = frequency[frequency$author == "Donald J. Trump",],
         ~ other + obama)
cor.test(data = frequency[frequency$author == "William J. Clinton",], 
         ~ other + obama)
```
# It turns out the correlation between obama and trump is somewhat lower than between obama and clinton.  It's interesting to note that Obama and Clinton seem to mention themes like the "world" and "democracy" equally frequently

# Now let's look at some bigrams to see if there are any phrases that appear often
```{r}
clean_tb<-tb
clean_tb$text <-removePunctuation(tb$text,preserve_intra_word_dashes = TRUE)
clean_tb$text <-removeNumbers(tb$text)
clean_tb$text <-removeWords(tb$text,stopwords("english"))

word_bigram <- clean_tb %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

word_bigram
```
# Let's look at the most common bigrams
```{r}
word_bigram %>%
  count(bigram, sort = TRUE)
```
# There are a lot of "we"" and "I" constructions - this makes sense for a political speech.  What would happen if we could separate out these more generic phrases to see if there were more meaningful bigrams like "islamic terrorism"?

```{r, message=FALSE, warning=FALSE}
require(tidyr)
bigrams_separated <- word_bigram %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts
```
# There are some notable associations, like "protect, defend" and "constitution, laws" - this is perhaps due to the fact that these words are contained in the oath of office, which is included in some of the texts.

# Now we calculate tf-idf on bigrams for different parties to give us an idea of their most unique phrases (ones that are closely associated only with them)
```{r bigram_tf_idf, dependson = "bigram_counts"}
bigram_tf_idf <- bigrams_united %>%
  count(party, bigram) %>%
  bind_tf_idf(bigram, party, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf
```

# Let's plot differences in parties to see if there are any interesting trends.

```{r}

bigram_tf_idf_parties <- bind_rows(a <- filter(bigram_tf_idf, party=="Republican"),
                                   b <- filter(bigram_tf_idf, party=="Democratic"))

bigram_tf_idf_parties %>%
  arrange(desc(tf_idf)) %>%
  top_n(12, tf_idf) %>%
  ungroup() %>%
  mutate(bigram = reorder(bigram, tf_idf)) %>%
  ggplot(aes(bigram, tf_idf, fill = party)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ party, ncol = 2, scales = "free") +
  coord_flip() +
  labs(y = "tf-idf of bigram by party",
       x = "")
```
# These results are a little disappointing - from the plot we can see that there are a lot of bigrams that appear only once.  However, we have removed common words like "and", which has limited our ability to find matching terms.  What if we looked only for trigrams that had the structure [word] "and" [word]?  Maybe this would reveal difference between the parties in how they link concepts together.

# Let's construct the trigrams:
```{r}
trigram_tb<-tb
trigram_tb$text <-removePunctuation(tb$text,preserve_intra_word_dashes = TRUE)
trigram_tb$text <-removeNumbers(tb$text)

word_trigram <- trigram_tb %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3)

trigrams_separated <- word_trigram %>%
  separate(trigram, c("word1", "word2","word3"), sep = " ")

trigrams_filtered <- trigrams_separated %>%
  filter(word2 %in% "and")

trigrams_united <- trigrams_filtered %>%
  unite(trigram, word1, word2, word3, sep = " ")

# new trigram counts
trigram_counts <- trigrams_filtered %>% 
  count(word1, word2, word3, sort = TRUE)

trigram_tf_idf <- trigrams_united %>%
  count(party, trigram) %>%
  bind_tf_idf(trigram, party, n) %>%
  arrange(desc(tf_idf))

trigram_tf_idf_parties <- bind_rows(a <- filter(trigram_tf_idf, party=="Republican"),
                                   b <- filter(trigram_tf_idf, party=="Democratic"))

trigram_tf_idf_parties %>%
  arrange(desc(tf_idf)) %>%
  top_n(5, tf_idf) %>%
  ungroup() %>%
  mutate(trigram = reorder(trigram, tf_idf)) %>%
  ggplot(aes(trigram, tf_idf, fill = party)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ party, ncol = 2, scales = "free") +
  coord_flip() +
  labs(y = "tf-idf of trigram by party",
       x = "")
```
# let's look at "future" words to see if that reveals any difference in which words are used to convey positive sentiments.

```{r}

AFINN <- get_sentiments("afinn")

negation_words <- c("will", "shall", "never", "ever")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, score, sort = TRUE) %>%
  ungroup()

negation_words
```

```{r}
negated_words %>%
  mutate(contribution = n * score) %>%
  mutate(word2 = reorder(paste(word2, word1, sep = "__"), contribution)) %>%
  group_by(word1) %>%
  top_n(5, abs(contribution)) %>%
  ggplot(aes(word2, contribution, fill = n * score > 0)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ word1, scales = "free") +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  xlab("Words preceded by negation term") +
  ylab("Sentiment score * # of occurrences") +
  coord_flip()
```
# Now let's look at which foreign countries have been mentioned in inaugural addresses - surprisingly few!
```{r}
require('maps')
require('countrycode')
require('rworldmap')

countries = tolower(unique(world.cities$country.etc)) # get country names

combined_speeches = PlainTextDocument(speech.corpus,language = "eng")
regions = c("europe","asia","africa","north america","south america","middle east","central america")

tf.countries = termFreq(combined_speeches,control = list(dictionary = countries))
tf.regions = termFreq(combined_speeches,control = list(dictionary = regions))

countries_mentioned <- names(as.list(tf.countries[tf.countries!=0]))

theCountries <- countrycode(countries_mentioned,'country.name','iso3c')
# These are the ISO3 names of the countries you'd like to plot in red

menDF <- data.frame(country = theCountries,
  mentioned = TRUE)

menMap <- joinCountryData2Map(menDF, joinCode = "ISO3",
  nameJoinColumn = "country")
# This will join your menDF data.frame to the country map data

mapCountryData(menMap, nameColumnToPlot="mentioned", catMethod = "categorical",
  missingCountryCol = gray(.8), mapTitle = "Foreign Countries Mentioned in Inaugural Addresses", addLegend = FALSE)
# And this will plot it, with the trick that the color palette's first
# color is red
```


